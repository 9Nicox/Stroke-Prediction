{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4029610,"sourceType":"datasetVersion","datasetId":2387714}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 5 - Explainability ","metadata":{}},{"cell_type":"markdown","source":"**Features importances**","metadata":{}},{"cell_type":"code","source":"# Pour Random Forest\nimportances = best_rf.feature_importances_\nfeat_names = X.columns\nfeat_imp_df = pd.DataFrame({'feature': feat_names, 'importance': importances}).sort_values(by='importance', ascending=False)\n\n# Barplot\nplt.figure(figsize=(10,6))\nsns.barplot(x='importance', y='feature', data=feat_imp_df, palette='viridis')\nplt.title(\"Feature Importances - Random Forest\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T13:27:56.661827Z","iopub.execute_input":"2025-09-19T13:27:56.662255Z","iopub.status.idle":"2025-09-19T13:27:57.271883Z","shell.execute_reply.started":"2025-09-19T13:27:56.662213Z","shell.execute_reply":"2025-09-19T13:27:57.270781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shap\n\n# Explainer pour Random Forest\nexplainer = shap.TreeExplainer(best_rf)\nshap_values = explainer.shap_values(X_test)\n\n# Summary plot (global view)\nshap.summary_plot(shap_values[1], X_test)  # [1] = classe positive (stroke)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T13:27:57.273222Z","iopub.execute_input":"2025-09-19T13:27:57.273549Z","iopub.status.idle":"2025-09-19T13:28:04.489938Z","shell.execute_reply.started":"2025-09-19T13:27:57.27352Z","shell.execute_reply":"2025-09-19T13:28:04.488884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example for the first test set\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X_test.iloc[0,:])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T13:28:04.491164Z","iopub.execute_input":"2025-09-19T13:28:04.491677Z","iopub.status.idle":"2025-09-19T13:28:04.514244Z","shell.execute_reply.started":"2025-09-19T13:28:04.491649Z","shell.execute_reply":"2025-09-19T13:28:04.513197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The feature importances and SHAP plots reveal that age, **average glucose level, and BMI** are the most influential factors in predicting stroke. The SHAP summary plot confirms this globally, while the force plot illustrates how these features combine to influence the prediction for individual patients.\n\n**Importance of interpretability in medical applications:**\nIn healthcare, model predictions **are not enough on their own** — clinicians need to understand why a prediction was made to trust it and take action. Interpretability techniques, like feature importances and SHAP, provide insights into which patient characteristics drive the prediction. This ensures transparency, allows for clinical validation, and helps prevent decisions based solely on “black-box” models. Especially in critical scenarios such as stroke detection, this level of **explainability is essential**for patient safety, ethical responsibility, and regulatory compliance.\n\nOverall, combining high-performing predictive models with clear interpretability makes this approach suitable for clinical decision support systems, while maintaining trust and accountability","metadata":{}},{"cell_type":"markdown","source":"# 6 - What to conclude ?\n\nThis notebook shows us how to work on a classification problem in the medical field. We were able to see that it was fairly easy to find a model that performed very well in terms of accuracy (with a tendency for the model to classify all samples in class 0, i.e. Non-Stroke). But if there's one thing to take away from this Notebook, it's that it's important to ask ourselves what we want our model to achieve.\n\nIn the context of a predictive model in the medical field, accuracy may not be our most interesting metric: our main aim would be to ensure that everyone at risk of developing a stroke is detected, even if this means generating more false positives.\n\nAre our models perfect? Obviously not, and there are certainly other things that could be done to improve this model still further. But the main point here was to highlight the importance of looking at all, or at least more than one metric, and of clearly understanding what capability we are looking for in our classification model.","metadata":{}}]}